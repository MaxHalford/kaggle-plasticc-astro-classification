{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>mjd</th>\n",
       "      <th>passband</th>\n",
       "      <th>flux</th>\n",
       "      <th>flux_err</th>\n",
       "      <th>detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4229</td>\n",
       "      <td>2</td>\n",
       "      <td>-544.810303</td>\n",
       "      <td>3.622952</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4306</td>\n",
       "      <td>1</td>\n",
       "      <td>-816.434326</td>\n",
       "      <td>5.553370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4383</td>\n",
       "      <td>3</td>\n",
       "      <td>-471.385529</td>\n",
       "      <td>3.801213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4450</td>\n",
       "      <td>4</td>\n",
       "      <td>-388.984985</td>\n",
       "      <td>11.395031</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>615</td>\n",
       "      <td>59752.4070</td>\n",
       "      <td>2</td>\n",
       "      <td>-681.858887</td>\n",
       "      <td>4.041204</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id         mjd  passband        flux   flux_err  detected\n",
       "0        615  59750.4229         2 -544.810303   3.622952         1\n",
       "1        615  59750.4306         1 -816.434326   5.553370         1\n",
       "2        615  59750.4383         3 -471.385529   3.801213         1\n",
       "3        615  59750.4450         4 -388.984985  11.395031         1\n",
       "4        615  59752.4070         2 -681.858887   4.041204         1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/training_set.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Todo: https://arxiv.org/pdf/1711.10609.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def generate_series(paths, prepare, chunk_size=1e6):\n",
    "    \n",
    "    # Read the data in chunks\n",
    "    chunks = itertools.chain(*[pd.read_csv(p, chunksize=chunk_size) for p in paths])\n",
    "    orphans = pd.DataFrame()\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        \n",
    "        # Add the previous orphans to the chunk\n",
    "        chunk = pd.concat((orphans, chunk))\n",
    "        \n",
    "        # Determine which rows are orphans\n",
    "        last_val = chunk['object_id'].iloc[-1]\n",
    "        is_orphan = chunk['object_id'] == last_val\n",
    "        \n",
    "        # Put the new orphans aside\n",
    "        chunk, orphans = chunk[~is_orphan], chunk[is_orphan]\n",
    "        \n",
    "        # Yield one series per object\n",
    "        for object_id, g in chunk.groupby('object_id'):\n",
    "            yield object_id, prepare(g)\n",
    "                \n",
    "                \n",
    "def generate_batch_series(paths, prepare, batch_size=16, chunk_size=1e6):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        batch = []\n",
    "        \n",
    "        for object_id, series in generate_series(paths, prepare, chunk_size=1e6):\n",
    "            batch.append(series.reshape(-1, 1))\n",
    "        \n",
    "            if len(batch) == batch_size:\n",
    "                yield np.array(batch), np.array(batch)\n",
    "                batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers as l\n",
    "from keras import models as m\n",
    "\n",
    "timesteps = 180\n",
    "encoding_dim = 16 \n",
    "\n",
    "inputs = l.Input(shape=(timesteps, 1))\n",
    "encoded = l.normalization.BatchNormalization()(inputs)\n",
    "encoded = l.LSTM(units=16)(encoded)\n",
    "\n",
    "decoded = l.RepeatVector(timesteps)(encoded)\n",
    "decoded = l.LSTM(1, return_sequences=True)(decoded)\n",
    "\n",
    "autoencoder = m.Model(inputs, decoded)\n",
    "encoder = m.Model(inputs, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 4.9697\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 221s 221ms/step - loss: 1.5871\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 180s 180ms/step - loss: 4.5011\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 180s 180ms/step - loss: 8.1312\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 180s 180ms/step - loss: 8.2431\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 181s 181ms/step - loss: 8.0960\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 181s 181ms/step - loss: 7.9786\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 182s 182ms/step - loss: 8.0990\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 183s 183ms/step - loss: 8.1053\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 183s 183ms/step - loss: 8.0283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2df5441358>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log(x):\n",
    "    y = np.log1p(np.abs(x))\n",
    "    return np.where(x < 0, -y, y)\n",
    "\n",
    "\n",
    "def prepare_series(g):\n",
    "    return log(np.interp(x=np.linspace(59580, 60674, timesteps), xp=g['mjd'], fp=g['flux']))\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "generator = generate_batch_series(paths=['data/training_set.csv', 'data/test_set.csv'], prepare=prepare_series, batch_size=batch_size)\n",
    "\n",
    "autoencoder.fit_generator(generator, steps_per_epoch=1000, epochs=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generate_series(paths=['data/training_set.csv', 'data/test_set.csv'], prepare=prepare_series)\n",
    "\n",
    "features = {}\n",
    "ids = []\n",
    "batch = []\n",
    "\n",
    "for object_id, series in generator:\n",
    "    \n",
    "    ids.append(object_id)\n",
    "    batch.append(series.reshape(-1, 1))\n",
    "    \n",
    "    if len(batch) == batch_size:\n",
    "        encoded = encoder.predict(np.array(batch))\n",
    "        for i, enc in zip(ids, encoded):\n",
    "            features[i] = enc\n",
    "        ids = []\n",
    "        batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(features, orient='index').add_prefix('auto_').to_hdf('data/features.h5', 'autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
